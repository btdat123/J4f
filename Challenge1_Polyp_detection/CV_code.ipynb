{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch==0.3.3 torch torchvision albumentations==1.4.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POLYP SEGMENTATION - Attention U-Net ResNet34 (Improved + Best IoU Save)\n",
    "-----------------------------------------------------------------------\n",
    "C·∫£i ti·∫øn: Attention U-Net + Dice Loss + Albumentations + Cosine LR + Save Best IoU.\n",
    "D·ªØ li·ªáu: /MyDrive/polyp_data/\n",
    "\"\"\"\n",
    "\n",
    "# =========================================================\n",
    "# 0Ô∏è‚É£ Mount Drive\n",
    "# =========================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_PATH = '/content/drive/MyDrive/polyp_data'\n",
    "SAVE_PATH = '/content/drive/MyDrive/polyp_results'\n",
    "\n",
    "import os, zipfile, numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# 1Ô∏è‚É£ Dataset Loader\n",
    "# =========================================================\n",
    "class PolypDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir=None, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "        self.has_mask = mask_dir is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.has_mask:\n",
    "            mask_path = os.path.join(self.mask_dir, img_name)\n",
    "            mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "            mask = (mask > 127).astype(np.float32)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        if self.transform:\n",
    "            if mask is not None:\n",
    "                augmented = self.transform(image=image, mask=mask)\n",
    "                image, mask = augmented[\"image\"], augmented[\"mask\"].unsqueeze(0)\n",
    "            else:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented[\"image\"]\n",
    "\n",
    "        return (image, mask) if self.has_mask else (image, img_name)\n",
    "\n",
    "# =========================================================\n",
    "# 2Ô∏è‚É£ Data Augmentation\n",
    "# =========================================================\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# =========================================================\n",
    "# 3Ô∏è‚É£ Attention U-Net ResNet34\n",
    "# =========================================================\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, 1, 1, 0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, 1, 1, 0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, 1, 1, 0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return psi\n",
    "\n",
    "\n",
    "class AttentionUNetResNet34(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
    "        self.encoder0 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n",
    "        self.encoder1 = backbone.layer1\n",
    "        self.encoder2 = backbone.layer2\n",
    "        self.encoder3 = backbone.layer3\n",
    "        self.encoder4 = backbone.layer4\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.att4 = AttentionBlock(256, 512, 128)\n",
    "        self.dec4 = self._decoder_block(256 + 512, 256)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.att3 = AttentionBlock(128, 256, 64)\n",
    "        self.dec3 = self._decoder_block(128 + 256, 128)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.att2 = AttentionBlock(64, 128, 32)\n",
    "        self.dec2 = self._decoder_block(64 + 128, 64)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, 2, 2)\n",
    "        self.att1 = AttentionBlock(64, 64, 16)\n",
    "        self.dec1 = self._decoder_block(64 + 64, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def _decoder_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e0 = self.encoder0(x)\n",
    "        e1 = self.encoder1(e0)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        c = self.center(e4)\n",
    "\n",
    "        u4 = self.upconv4(c)\n",
    "        u4_resized = nn.functional.interpolate(u4, size=e4.shape[2:], mode='bilinear', align_corners=True)\n",
    "        att4_map = self.att4(u4_resized, e4)\n",
    "        e4_att = e4 * att4_map\n",
    "        e4_att_resized = nn.functional.interpolate(e4_att, size=u4.shape[2:], mode='bilinear', align_corners=True)\n",
    "        d4 = self.dec4(torch.cat([u4, e4_att_resized], 1))\n",
    "\n",
    "        u3 = self.upconv3(d4)\n",
    "        u3_resized = nn.functional.interpolate(u3, size=e3.shape[2:], mode='bilinear', align_corners=True)\n",
    "        att3_map = self.att3(u3_resized, e3)\n",
    "        e3_att = e3 * att3_map\n",
    "        e3_att_resized = nn.functional.interpolate(e3_att, size=u3.shape[2:], mode='bilinear', align_corners=True)\n",
    "        d3 = self.dec3(torch.cat([u3, e3_att_resized], 1))\n",
    "\n",
    "        u2 = self.upconv2(d3)\n",
    "        u2_resized = nn.functional.interpolate(u2, size=e2.shape[2:], mode='bilinear', align_corners=True)\n",
    "        att2_map = self.att2(u2_resized, e2)\n",
    "        e2_att = e2 * att2_map\n",
    "        e2_att_resized = nn.functional.interpolate(e2_att, size=u2.shape[2:], mode='bilinear', align_corners=True)\n",
    "        d2 = self.dec2(torch.cat([u2, e2_att_resized], 1))\n",
    "\n",
    "        u1 = self.upconv1(d2)\n",
    "        u1_resized = nn.functional.interpolate(u1, size=e1.shape[2:], mode='bilinear', align_corners=True)\n",
    "        att1_map = self.att1(u1_resized, e1)\n",
    "        e1_att = e1 * att1_map\n",
    "        e1_att_resized = nn.functional.interpolate(e1_att, size=u1.shape[2:], mode='bilinear', align_corners=True)\n",
    "        d1 = self.dec1(torch.cat([u1, e1_att_resized], 1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "# =========================================================\n",
    "# 4Ô∏è‚É£ Loss & Metrics\n",
    "# =========================================================\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, targets):\n",
    "        preds = torch.sigmoid(logits)\n",
    "        intersection = (preds * targets).sum()\n",
    "        dice = (2 * intersection + self.smooth) / (preds.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "def compute_iou_dice(preds, masks):\n",
    "    preds = (torch.sigmoid(preds) > 0.5).float()\n",
    "    intersection = (preds * masks).sum()\n",
    "    union = preds.sum() + masks.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    dice = (2 * intersection + 1e-6) / (preds.sum() + masks.sum() + 1e-6)\n",
    "    return iou.item(), dice.item()\n",
    "\n",
    "# =========================================================\n",
    "# 5Ô∏è‚É£ Training (Save best IoU)\n",
    "# =========================================================\n",
    "def train_model(model, train_loader, val_loader, device, epochs=15, lr=1e-4):\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    dice = DiceLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.to(device)\n",
    "\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            loss = 0.5*bce(preds, masks) + 0.5*dice(preds, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        # üîç Validation\n",
    "        model.eval()\n",
    "        val_iou, val_dice = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                preds = model(imgs)\n",
    "                iou, d = compute_iou_dice(preds, masks)\n",
    "                val_iou += iou\n",
    "                val_dice += d\n",
    "        val_iou /= len(val_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "\n",
    "        print(f\"‚úÖ Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, IoU={val_iou:.4f}, Dice={val_dice:.4f}\")\n",
    "\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), os.path.join(SAVE_PATH, \"best_model_iou.pth\"))\n",
    "            print(f\"üíæ Saved best IoU model (IoU={best_iou:.4f})\")\n",
    "\n",
    "# =========================================================\n",
    "# 6Ô∏è‚É£ Inference (TTA + ZIP)\n",
    "# =========================================================\n",
    "def predict_masks(model, loader, device, zip_path):\n",
    "    model.eval()\n",
    "    pred_dir = os.path.join(SAVE_PATH, \"pred_masks\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, names in tqdm(loader, desc=\"Predicting\"):\n",
    "            imgs = imgs.to(device)\n",
    "            preds1 = torch.sigmoid(model(imgs))\n",
    "            preds2 = torch.sigmoid(model(torch.flip(imgs, dims=[3])))  # Horizontal flip\n",
    "            preds = (preds1 + torch.flip(preds2, dims=[3])) / 2\n",
    "            preds = (preds > 0.5).float().cpu().numpy()\n",
    "\n",
    "            for i, name in enumerate(names):\n",
    "                mask = (preds[i, 0] * 255).astype(np.uint8)\n",
    "                Image.fromarray(mask).save(os.path.join(pred_dir, name.replace(\".jpg\", \".png\")))\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "        for f in os.listdir(pred_dir):\n",
    "            zipf.write(os.path.join(pred_dir, f), arcname=f)\n",
    "    print(f\"üéØ Saved masks to: {zip_path}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7Ô∏è‚É£ Main\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = PolypDataset(\n",
    "        os.path.join(DRIVE_PATH, \"train/images\"),\n",
    "        os.path.join(DRIVE_PATH, \"train/masks\"),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = AttentionUNetResNet34(pretrained=True)\n",
    "    train_model(model, train_loader, val_loader, device, epochs=300, lr=1e-4)\n",
    "\n",
    "    # D·ª± ƒëo√°n\n",
    "    test_ds = PolypDataset(\n",
    "        os.path.join(DRIVE_PATH, \"images-public/images\"),\n",
    "        mask_dir=None,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "    model.load_state_dict(torch.load(os.path.join(SAVE_PATH, \"best_model_iou.pth\"), map_location=device))\n",
    "\n",
    "    output_zip = os.path.join(SAVE_PATH, \"pred_mask.zip\")\n",
    "    predict_masks(model, test_loader, device, output_zip)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
